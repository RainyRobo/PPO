{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RainyRobo/PPO/blob/main/PPO_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4VJcUT2GlJz"
      },
      "source": [
        "################################################################################\n",
        "> # **Install Dependencies**\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbpSQTflGlAr"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "############ install compatible version of OpenAI roboschool and gym ###########\n",
        "\n",
        "!pip install swig\n",
        "\n",
        "# !pip install roboschool==1.0.7 gym==0.15.4\n",
        "\n",
        "# !pip install box2d-py\n",
        "\n",
        "# !pip install Box2D\n",
        "\n",
        "# !pip install pybullet\n",
        "\n",
        "# !pip install gym[box2d]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzZairIiGQ11"
      },
      "source": [
        "################################################################################\n",
        "> # **Introduction**\n",
        "> The notebook is divided into 5 major parts :\n",
        "\n",
        "*   **Part I** : define actor-critic network and PPO algorithm\n",
        "*   **Part II** : train PPO algorithm and save network weights and log files\n",
        "*   **Part III** : load (preTrained) network weights and test PPO algorithm\n",
        "*   **Part IV** : load log files and plot graphs\n",
        "*   **Part V** : install xvbf, load (preTrained) network weights and save images for gif and then generate gif\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s37cJXAYGrTY"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - I**\n",
        "\n",
        "*   define actor critic networks\n",
        "*   define PPO algorithm\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############################### Import libraries ###############################\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import MultivariateNormal\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import gym\n",
        "# import roboschool\n",
        "import pybullet_envs\n",
        "\n",
        "\n",
        "################################## set device ##################################\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "# set device to cpu or cuda\n",
        "device = torch.device('cpu')\n",
        "\n",
        "if(torch.cuda.is_available()):\n",
        "    device = torch.device('cuda:0')\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
        "else:\n",
        "    print(\"Device set to : cpu\")\n",
        "\n",
        "print(\"============================================================================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqGpUFxbe2Gb",
        "outputId": "ca33cc7a-21bc-4e51-850d-a11f58b2034f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "Device set to : cpu\n",
            "============================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################## PPO Policy ##################################\n",
        "\n",
        "class RolloutBuffer:\n",
        "    def __init__(self):\n",
        "        self.actions = []\n",
        "        self.states = []\n",
        "        self.logprobs = [] #表示动作的对数概率列表为空\n",
        "        self.rewards = []\n",
        "        self.state_values = []\n",
        "        self.is_terminals = []\n",
        "\n",
        "\n",
        "    def clear(self):\n",
        "        del self.actions[:]\n",
        "        del self.states[:]\n",
        "        del self.logprobs[:]\n",
        "        del self.rewards[:]\n",
        "        del self.state_values[:]\n",
        "        del self.is_terminals[:]"
      ],
      "metadata": {
        "id": "F4bX4u9wm0uJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# buffer = RolloutBuffer()\n",
        "# for i in range(10):\n",
        "#   buffer.actions.append(i)\n",
        "# print(\"Actions:\", buffer.actions)\n",
        "\n",
        "# buffer.clear()\n",
        "# print(\"Clear Actions:\", buffer.actions)"
      ],
      "metadata": {
        "id": "-g1QmKHRn4_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A2C Model\n",
        "class ActorCritic(nn.Module):\n",
        "\n",
        "    # action_dim: 动作维度表示你需要控制的独立变量的数量。在 Manipulation（如机械臂操控）任务中，这些变量通常与机器人的自由度（关节的数量）或末端执行器的控制变量（位置、方向等）有关。\n",
        "    # state_dim: 1）环境的状态；2）机器人的状态；···\n",
        "    def __init__(self, state_dim, action_dim, has_continuous_action_space, action_std_init):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        self.has_continuous_action_space = has_continuous_action_space\n",
        "\n",
        "        # 为什么区分是否为连续的动作空间\n",
        "\n",
        "        # 1.动作空间的性质\n",
        "          # 离散动作空间：离散动作空间包含有限的离散动作。例如，机器人可以选择“向前”、“向后”、“左转”或“右转”等动作。每个动作可以被视为一个离散的类别，通常表示为整数。\n",
        "          # 连续动作空间：连续动作空间则包含无限多个可能的动作，动作可以是任何实数值。比如，机器人控制舵机的角度或车辆的速度，动作可能是从 -1 到 1 之间的任何一个实数值。\n",
        "\n",
        "        # 2.模型输出\n",
        "          # 离散动作空间：通常使用 概率分布 来表示每个可能动作的选择概率。常见的做法是通过一个 Softmax 函数来输出每个离散动作的概率。\n",
        "          # 连续动作空间：对于连续动作空间，通常使用 概率分布的参数（如均值和方差）来表示动作的概率分布。常见做法是使用 高斯分布 或其他分布来建模动作。例如，输出的网络可以生成一个动作的均值和标准差，然后从该分布中采样得到具体的动作。\n",
        "\n",
        "        if has_continuous_action_space:\n",
        "            self.action_dim = action_dim\n",
        "\n",
        "            # torch.full()：是 PyTorch 中用来创建一个张量（Tensor）的函数，它的作用是创建一个指定形状的张量，并且填充该张量的所有元素为一个指定的值。\n",
        "            # torch.full(size, fill_value)\n",
        "            # (action_dim,)：（列，行）空为0\n",
        "            self.action_var = torch.full(\n",
        "                (action_dim,),action_std_init * action_std_init).to(device)\n",
        "\n",
        "        # actor：策略网络\n",
        "        if has_continuous_action_space :\n",
        "            # from state to action (state -> action)\n",
        "            self.actor = nn.Sequential(\n",
        "                            nn.Linear(state_dim, 64),#输入层：将状态维度映射到 64\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(64, 64),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(64, action_dim),\n",
        "                            nn.Tanh()\n",
        "                        )\n",
        "        # 离散动作空间\n",
        "        else:\n",
        "            self.actor = nn.Sequential(\n",
        "                            nn.Linear(state_dim, 64),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(64, 64),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(64, action_dim),\n",
        "                            nn.Softmax(dim=-1) #这层为区别连续/离散动作空间的关键：类似于分类任务，对应到具体的动作类别上\n",
        "                        )\n",
        "\n",
        "\n",
        "        # critic：值网络（value-based）\n",
        "        # 用于评估当前（state）下执行当前（action）的价值，Value（state,action）\n",
        "        # Value（state,action）两种训练方法：\n",
        "        # 1）蒙特卡洛方法：1)最简单就是从头跑到尾得到（state,action）->reward as critic；\n",
        "        # 2)利用相邻的reward的时间差分法*:\n",
        "        self.critic = nn.Sequential(\n",
        "                        nn.Linear(state_dim, 64),\n",
        "                        nn.Tanh(),\n",
        "                        nn.Linear(64, 64),\n",
        "                        nn.Tanh(),\n",
        "                        nn.Linear(64, 1)\n",
        "                    )\n",
        "\n",
        "    # 动作标准差更新\n",
        "    def set_action_std(self, new_action_std):\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std).to(device)\n",
        "        else:\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "            print(\"WARNING : Calling ActorCritic::set_action_std() on discrete action space policy\")\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "    # forward 方法：在 ActorCritic 中并未被实现，因为我们不需要通过 forward 进行推理。\n",
        "\n",
        "    def forward(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    # act 方法：作为实际的推理方法，它实现了类似于前向传播的操作，但同时还结合了强化学习中的策略选择、奖励计算和价值评估。\n",
        "    def act(self, state):\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            action_mean = self.actor(state) #action_mean是一个向量，表示每个动作维度的均值。\n",
        "\n",
        "            # 协方差矩阵，用于表示多元高斯分布的方差：\n",
        "            # torch.diag(self.action_var)：生成一个对角矩阵，其对角线上的元素来自 self.action_var，非对角线元素为零。\n",
        "            # .unsqueeze(dim=0)：\n",
        "              # 假设 cov_mat 是一个大小为 (3, 3) 的对角矩阵：\n",
        "              # 使用 unsqueeze(dim=0) 后，得到的张量形状变为 (1, 3, 3)：\n",
        "              # 这是因为多元高斯分布的协方差矩阵要求的形状是 (batch_size, n, n)，其中 n 是动作的维度。unsqueeze 让它符合这一形状。\n",
        "            cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n",
        "\n",
        "            # 1.MultivariateNormal：用于在强化学习中生成符合多维正态分布的随机变量，特别是在连续动作空间的策略中。\n",
        "            # 目前的简单理解就是将对于每个节点的动作概率分布绘制在一起快，从二维变成三维度\n",
        "            dist = MultivariateNormal(action_mean, cov_mat)\n",
        "\n",
        "        # 离散型\n",
        "        else:\n",
        "            action_probs = self.actor(state) #得到了softmax结果，每个类别的概率值\n",
        "            dist = Categorical(action_probs) #构建离散概率分布\n",
        "\n",
        "        # 你可能问为什么再次构建一次连续/离散的概率分布？\n",
        "        # 因为利用MultivariateNormal/Categorical是已经定义好的类，封装好了很多如sample函数\n",
        "\n",
        "        action = dist.sample() #从分布中采样\n",
        "        action_logprob = dist.log_prob(action) #它计算某个动作被选择的对数概率。\n",
        "        state_val = self.critic(state)\n",
        "\n",
        "        # detach() 的作用是使得 action 张量不再参与后续的梯度计算，从而 防止它影响策略梯度的计算。\n",
        "        return action.detach(), action_logprob.detach(), state_val.detach()\n",
        "\n",
        "\n",
        "    def evaluate(self, state, action):\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            action_mean = self.actor(state)\n",
        "            action_var = self.action_var.expand_as(action_mean)\n",
        "            cov_mat = torch.diag_embed(action_var).to(device)\n",
        "            dist = MultivariateNormal(action_mean, cov_mat)\n",
        "\n",
        "            # for single action continuous environments\n",
        "            if self.action_dim == 1:\n",
        "                action = action.reshape(-1, self.action_dim)\n",
        "\n",
        "        else:\n",
        "            action_probs = self.actor(state)\n",
        "            dist = Categorical(action_probs)\n",
        "\n",
        "        action_logprobs = dist.log_prob(action)\n",
        "        dist_entropy = dist.entropy()\n",
        "        state_values = self.critic(state)\n",
        "\n",
        "        return action_logprobs, state_values, dist_entropy"
      ],
      "metadata": {
        "id": "tkSOtPNom5fz",
        "outputId": "efa8aaa2-4f56-4f1a-90d3-affb319911d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actions: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Clear Actions: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UT6VUBg-F8Zm"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "class PPO:\n",
        "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std_init=0.6):\n",
        "\n",
        "        self.has_continuous_action_space = has_continuous_action_space\n",
        "\n",
        "        if has_continuous_action_space:\n",
        "            self.action_std = action_std_init\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.K_epochs = K_epochs\n",
        "\n",
        "        self.buffer = RolloutBuffer()\n",
        "\n",
        "        self.policy = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
        "        self.optimizer = torch.optim.Adam([\n",
        "                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
        "                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
        "                    ])\n",
        "\n",
        "        self.policy_old = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        self.MseLoss = nn.MSELoss()\n",
        "\n",
        "\n",
        "    def set_action_std(self, new_action_std):\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            self.action_std = new_action_std\n",
        "            self.policy.set_action_std(new_action_std)\n",
        "            self.policy_old.set_action_std(new_action_std)\n",
        "\n",
        "        else:\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "            print(\"WARNING : Calling PPO::set_action_std() on discrete action space policy\")\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "    def decay_action_std(self, action_std_decay_rate, min_action_std):\n",
        "        print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            self.action_std = self.action_std - action_std_decay_rate\n",
        "            self.action_std = round(self.action_std, 4)\n",
        "            if (self.action_std <= min_action_std):\n",
        "                self.action_std = min_action_std\n",
        "                print(\"setting actor output action_std to min_action_std : \", self.action_std)\n",
        "            else:\n",
        "                print(\"setting actor output action_std to : \", self.action_std)\n",
        "            self.set_action_std(self.action_std)\n",
        "\n",
        "        else:\n",
        "            print(\"WARNING : Calling PPO::decay_action_std() on discrete action space policy\")\n",
        "\n",
        "        print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "    def select_action(self, state):\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            with torch.no_grad():\n",
        "                state = torch.FloatTensor(state).to(device)\n",
        "                action, action_logprob, state_val = self.policy_old.act(state)\n",
        "\n",
        "            self.buffer.states.append(state)\n",
        "            self.buffer.actions.append(action)\n",
        "            self.buffer.logprobs.append(action_logprob)\n",
        "            self.buffer.state_values.append(state_val)\n",
        "\n",
        "            return action.detach().cpu().numpy().flatten()\n",
        "\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                state = torch.FloatTensor(state).to(device)\n",
        "                action, action_logprob, state_val = self.policy_old.act(state)\n",
        "\n",
        "            self.buffer.states.append(state)\n",
        "            self.buffer.actions.append(action)\n",
        "            self.buffer.logprobs.append(action_logprob)\n",
        "            self.buffer.state_values.append(state_val)\n",
        "\n",
        "            return action.item()\n",
        "\n",
        "\n",
        "    def update(self):\n",
        "\n",
        "        # Monte Carlo estimate of returns\n",
        "        rewards = []\n",
        "        discounted_reward = 0\n",
        "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
        "            if is_terminal:\n",
        "                discounted_reward = 0\n",
        "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
        "            rewards.insert(0, discounted_reward)\n",
        "\n",
        "        # Normalizing the rewards\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
        "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
        "\n",
        "        # convert list to tensor\n",
        "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
        "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
        "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n",
        "        old_state_values = torch.squeeze(torch.stack(self.buffer.state_values, dim=0)).detach().to(device)\n",
        "\n",
        "        # calculate advantages\n",
        "        advantages = rewards.detach() - old_state_values.detach()\n",
        "\n",
        "\n",
        "        # Optimize policy for K epochs\n",
        "        for _ in range(self.K_epochs):\n",
        "\n",
        "            # Evaluating old actions and values\n",
        "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
        "\n",
        "            # match state_values tensor dimensions with rewards tensor\n",
        "            state_values = torch.squeeze(state_values)\n",
        "\n",
        "            # Finding the ratio (pi_theta / pi_theta__old)\n",
        "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
        "\n",
        "            # Finding Surrogate Loss\n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
        "\n",
        "            # final loss of clipped objective PPO\n",
        "            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * dist_entropy\n",
        "\n",
        "            # take gradient step\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        # Copy new weights into old policy\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        # clear buffer\n",
        "        self.buffer.clear()\n",
        "\n",
        "\n",
        "    def save(self, checkpoint_path):\n",
        "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
        "\n",
        "\n",
        "    def load(self, checkpoint_path):\n",
        "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
        "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xCb_EyxF8cF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "################################# End of Part I ################################\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr-ZjT_CGyEi"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - II**\n",
        "\n",
        "*   train PPO algorithm on environments\n",
        "*   save preTrained networks weights and log files\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY1-DzVCF8eh",
        "outputId": "a624165d-8e91-4e2d-ad59-d6f184ee92fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "training environment name : CartPole-v1\n",
            "current logging run number for CartPole-v1 :  10\n",
            "logging at : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_10.csv\n",
            "save checkpoint path : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "--------------------------------------------------------------------------------------------\n",
            "max training timesteps :  100000\n",
            "max timesteps per episode :  400\n",
            "model saving frequency : 20000 timesteps\n",
            "log frequency : 800 timesteps\n",
            "printing average reward over episodes in last : 1600 timesteps\n",
            "--------------------------------------------------------------------------------------------\n",
            "state space dimension :  4\n",
            "action space dimension :  2\n",
            "--------------------------------------------------------------------------------------------\n",
            "Initializing a discrete action space policy\n",
            "--------------------------------------------------------------------------------------------\n",
            "PPO update frequency : 1600 timesteps\n",
            "PPO K epochs :  40\n",
            "PPO epsilon clip :  0.2\n",
            "discount factor (gamma) :  0.99\n",
            "--------------------------------------------------------------------------------------------\n",
            "optimizer learning rate actor :  0.0003\n",
            "optimizer learning rate critic :  0.001\n",
            "============================================================================================\n",
            "Started training at (GMT) :  2025-05-12 08:22:07\n",
            "============================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode : 65 \t\t Timestep : 1600 \t\t Average Reward : 24.51\n",
            "Episode : 116 \t\t Timestep : 3200 \t\t Average Reward : 30.76\n",
            "Episode : 156 \t\t Timestep : 4800 \t\t Average Reward : 40.92\n",
            "Episode : 186 \t\t Timestep : 6400 \t\t Average Reward : 51.93\n",
            "Episode : 204 \t\t Timestep : 8000 \t\t Average Reward : 87.06\n",
            "Episode : 213 \t\t Timestep : 9600 \t\t Average Reward : 170.22\n",
            "Episode : 223 \t\t Timestep : 11200 \t\t Average Reward : 167.0\n",
            "Episode : 233 \t\t Timestep : 12800 \t\t Average Reward : 158.7\n",
            "Episode : 241 \t\t Timestep : 14400 \t\t Average Reward : 187.62\n",
            "Episode : 248 \t\t Timestep : 16000 \t\t Average Reward : 229.14\n",
            "Episode : 255 \t\t Timestep : 17600 \t\t Average Reward : 229.0\n",
            "Episode : 261 \t\t Timestep : 19200 \t\t Average Reward : 235.67\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:00:30\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 268 \t\t Timestep : 20800 \t\t Average Reward : 269.57\n",
            "Episode : 274 \t\t Timestep : 22400 \t\t Average Reward : 233.17\n",
            "Episode : 280 \t\t Timestep : 24000 \t\t Average Reward : 285.0\n",
            "Episode : 287 \t\t Timestep : 25600 \t\t Average Reward : 251.43\n",
            "Episode : 291 \t\t Timestep : 27200 \t\t Average Reward : 370.75\n",
            "Episode : 295 \t\t Timestep : 28800 \t\t Average Reward : 339.0\n",
            "Episode : 300 \t\t Timestep : 30400 \t\t Average Reward : 339.2\n",
            "Episode : 304 \t\t Timestep : 32000 \t\t Average Reward : 376.75\n",
            "Episode : 309 \t\t Timestep : 33600 \t\t Average Reward : 387.0\n",
            "Episode : 313 \t\t Timestep : 35200 \t\t Average Reward : 398.5\n",
            "Episode : 317 \t\t Timestep : 36800 \t\t Average Reward : 400.0\n",
            "Episode : 321 \t\t Timestep : 38400 \t\t Average Reward : 364.75\n",
            "Episode : 326 \t\t Timestep : 40000 \t\t Average Reward : 352.8\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:00:49\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 330 \t\t Timestep : 41600 \t\t Average Reward : 400.0\n",
            "Episode : 335 \t\t Timestep : 43200 \t\t Average Reward : 271.2\n",
            "Episode : 342 \t\t Timestep : 44800 \t\t Average Reward : 260.57\n",
            "Episode : 348 \t\t Timestep : 46400 \t\t Average Reward : 252.5\n",
            "Episode : 353 \t\t Timestep : 48000 \t\t Average Reward : 280.8\n",
            "Episode : 358 \t\t Timestep : 49600 \t\t Average Reward : 378.8\n",
            "Episode : 362 \t\t Timestep : 51200 \t\t Average Reward : 371.0\n",
            "Episode : 366 \t\t Timestep : 52800 \t\t Average Reward : 389.25\n",
            "Episode : 370 \t\t Timestep : 54400 \t\t Average Reward : 369.75\n",
            "Episode : 375 \t\t Timestep : 56000 \t\t Average Reward : 366.4\n",
            "Episode : 379 \t\t Timestep : 57600 \t\t Average Reward : 400.0\n",
            "Episode : 384 \t\t Timestep : 59200 \t\t Average Reward : 294.4\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:01:08\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 389 \t\t Timestep : 60800 \t\t Average Reward : 333.0\n",
            "Episode : 393 \t\t Timestep : 62400 \t\t Average Reward : 389.5\n",
            "Episode : 398 \t\t Timestep : 64000 \t\t Average Reward : 302.0\n",
            "Episode : 403 \t\t Timestep : 65600 \t\t Average Reward : 334.0\n",
            "Episode : 407 \t\t Timestep : 67200 \t\t Average Reward : 396.5\n",
            "Episode : 411 \t\t Timestep : 68800 \t\t Average Reward : 400.0\n",
            "Episode : 415 \t\t Timestep : 70400 \t\t Average Reward : 400.0\n",
            "Episode : 419 \t\t Timestep : 72000 \t\t Average Reward : 400.0\n",
            "Episode : 423 \t\t Timestep : 73600 \t\t Average Reward : 400.0\n",
            "Episode : 427 \t\t Timestep : 75200 \t\t Average Reward : 400.0\n",
            "Episode : 431 \t\t Timestep : 76800 \t\t Average Reward : 400.0\n",
            "Episode : 435 \t\t Timestep : 78400 \t\t Average Reward : 373.0\n",
            "Episode : 442 \t\t Timestep : 80000 \t\t Average Reward : 268.86\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:01:27\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 446 \t\t Timestep : 81600 \t\t Average Reward : 351.5\n",
            "Episode : 450 \t\t Timestep : 83200 \t\t Average Reward : 400.0\n",
            "Episode : 454 \t\t Timestep : 84800 \t\t Average Reward : 400.0\n",
            "Episode : 458 \t\t Timestep : 86400 \t\t Average Reward : 400.0\n",
            "Episode : 462 \t\t Timestep : 88000 \t\t Average Reward : 400.0\n",
            "Episode : 468 \t\t Timestep : 89600 \t\t Average Reward : 302.33\n",
            "Episode : 472 \t\t Timestep : 91200 \t\t Average Reward : 400.0\n",
            "Episode : 477 \t\t Timestep : 92800 \t\t Average Reward : 313.4\n",
            "Episode : 481 \t\t Timestep : 94400 \t\t Average Reward : 400.0\n",
            "Episode : 485 \t\t Timestep : 96000 \t\t Average Reward : 372.75\n",
            "Episode : 491 \t\t Timestep : 97600 \t\t Average Reward : 268.67\n",
            "Episode : 495 \t\t Timestep : 99200 \t\t Average Reward : 337.25\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:01:46\n",
            "--------------------------------------------------------------------------------------------\n",
            "============================================================================================\n",
            "Started training at (GMT) :  2025-05-12 08:22:07\n",
            "Finished training at (GMT) :  2025-05-12 08:23:53\n",
            "Total training time  :  0:01:46\n",
            "============================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "################################### Training ###################################\n",
        "\n",
        "\n",
        "####### initialize environment hyperparameters ######\n",
        "\n",
        "env_name = \"CartPole-v1\"\n",
        "has_continuous_action_space = False\n",
        "\n",
        "max_ep_len = 400                    # max timesteps in one episode\n",
        "max_training_timesteps = int(1e5)   # break training loop if timeteps > max_training_timesteps\n",
        "\n",
        "print_freq = max_ep_len * 4     # print avg reward in the interval (in num timesteps)\n",
        "log_freq = max_ep_len * 2       # log avg reward in the interval (in num timesteps)\n",
        "save_model_freq = int(2e4)      # save model frequency (in num timesteps)\n",
        "\n",
        "action_std = None\n",
        "\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "## Note : print/log frequencies should be > than max_ep_len\n",
        "\n",
        "\n",
        "################ PPO hyperparameters ################\n",
        "\n",
        "\n",
        "update_timestep = max_ep_len * 4      # update policy every n timesteps\n",
        "K_epochs = 40               # update policy for K epochs\n",
        "eps_clip = 0.2              # clip parameter for PPO\n",
        "gamma = 0.99                # discount factor\n",
        "\n",
        "lr_actor = 0.0003       # learning rate for actor network\n",
        "lr_critic = 0.001       # learning rate for critic network\n",
        "\n",
        "random_seed = 0         # set random seed if required (0 = no random seed)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "\n",
        "print(\"training environment name : \" + env_name)\n",
        "\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# state space dimension\n",
        "state_dim = env.observation_space.shape[0]\n",
        "\n",
        "# action space dimension\n",
        "if has_continuous_action_space:\n",
        "    action_dim = env.action_space.shape[0]\n",
        "else:\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "\n",
        "\n",
        "###################### logging ######################\n",
        "\n",
        "#### log files for multiple runs are NOT overwritten\n",
        "\n",
        "log_dir = \"PPO_logs\"\n",
        "if not os.path.exists(log_dir):\n",
        "      os.makedirs(log_dir)\n",
        "\n",
        "log_dir = log_dir + '/' + env_name + '/'\n",
        "if not os.path.exists(log_dir):\n",
        "      os.makedirs(log_dir)\n",
        "\n",
        "\n",
        "#### get number of log files in log directory\n",
        "run_num = 0\n",
        "current_num_files = next(os.walk(log_dir))[2]\n",
        "run_num = len(current_num_files)\n",
        "\n",
        "\n",
        "#### create new log file for each run\n",
        "log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
        "\n",
        "print(\"current logging run number for \" + env_name + \" : \", run_num)\n",
        "print(\"logging at : \" + log_f_name)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "################### checkpointing ###################\n",
        "\n",
        "run_num_pretrained = 0      #### change this to prevent overwriting weights in same env_name folder\n",
        "\n",
        "directory = \"PPO_preTrained\"\n",
        "if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "\n",
        "directory = directory + '/' + env_name + '/'\n",
        "if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "\n",
        "\n",
        "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
        "print(\"save checkpoint path : \" + checkpoint_path)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "############# print all hyperparameters #############\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"max training timesteps : \", max_training_timesteps)\n",
        "print(\"max timesteps per episode : \", max_ep_len)\n",
        "\n",
        "print(\"model saving frequency : \" + str(save_model_freq) + \" timesteps\")\n",
        "print(\"log frequency : \" + str(log_freq) + \" timesteps\")\n",
        "print(\"printing average reward over episodes in last : \" + str(print_freq) + \" timesteps\")\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"state space dimension : \", state_dim)\n",
        "print(\"action space dimension : \", action_dim)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "if has_continuous_action_space:\n",
        "    print(\"Initializing a continuous action space policy\")\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "    print(\"starting std of action distribution : \", action_std)\n",
        "    print(\"decay rate of std of action distribution : \", action_std_decay_rate)\n",
        "    print(\"minimum std of action distribution : \", min_action_std)\n",
        "    print(\"decay frequency of std of action distribution : \" + str(action_std_decay_freq) + \" timesteps\")\n",
        "\n",
        "else:\n",
        "    print(\"Initializing a discrete action space policy\")\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"PPO update frequency : \" + str(update_timestep) + \" timesteps\")\n",
        "print(\"PPO K epochs : \", K_epochs)\n",
        "print(\"PPO epsilon clip : \", eps_clip)\n",
        "print(\"discount factor (gamma) : \", gamma)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"optimizer learning rate actor : \", lr_actor)\n",
        "print(\"optimizer learning rate critic : \", lr_critic)\n",
        "\n",
        "if random_seed:\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "    print(\"setting random seed to \", random_seed)\n",
        "    torch.manual_seed(random_seed)\n",
        "    env.seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "################# training procedure ################\n",
        "\n",
        "# initialize a PPO agent\n",
        "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
        "\n",
        "\n",
        "# track total training time\n",
        "start_time = datetime.now().replace(microsecond=0)\n",
        "print(\"Started training at (GMT) : \", start_time)\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "# logging file\n",
        "log_f = open(log_f_name,\"w+\")\n",
        "log_f.write('episode,timestep,reward\\n')\n",
        "\n",
        "\n",
        "# printing and logging variables\n",
        "print_running_reward = 0\n",
        "print_running_episodes = 0\n",
        "\n",
        "log_running_reward = 0\n",
        "log_running_episodes = 0\n",
        "\n",
        "time_step = 0\n",
        "i_episode = 0\n",
        "\n",
        "\n",
        "# training loop\n",
        "while time_step <= max_training_timesteps:\n",
        "\n",
        "    state = env.reset()\n",
        "    current_ep_reward = 0\n",
        "\n",
        "    for t in range(1, max_ep_len+1):\n",
        "\n",
        "        # select action with policy\n",
        "        action = ppo_agent.select_action(state)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "\n",
        "        # saving reward and is_terminals\n",
        "        ppo_agent.buffer.rewards.append(reward)\n",
        "        ppo_agent.buffer.is_terminals.append(done)\n",
        "\n",
        "        time_step +=1\n",
        "        current_ep_reward += reward\n",
        "\n",
        "        # update PPO agent\n",
        "        if time_step % update_timestep == 0:\n",
        "            ppo_agent.update()\n",
        "\n",
        "        # if continuous action space; then decay action std of ouput action distribution\n",
        "        if has_continuous_action_space and time_step % action_std_decay_freq == 0:\n",
        "            ppo_agent.decay_action_std(action_std_decay_rate, min_action_std)\n",
        "\n",
        "        # log in logging file\n",
        "        if time_step % log_freq == 0:\n",
        "\n",
        "            # log average reward till last episode\n",
        "            log_avg_reward = log_running_reward / log_running_episodes\n",
        "            log_avg_reward = round(log_avg_reward, 4)\n",
        "\n",
        "            log_f.write('{},{},{}\\n'.format(i_episode, time_step, log_avg_reward))\n",
        "            log_f.flush()\n",
        "\n",
        "            log_running_reward = 0\n",
        "            log_running_episodes = 0\n",
        "\n",
        "        # printing average reward\n",
        "        if time_step % print_freq == 0:\n",
        "\n",
        "            # print average reward till last episode\n",
        "            print_avg_reward = print_running_reward / print_running_episodes\n",
        "            print_avg_reward = round(print_avg_reward, 2)\n",
        "\n",
        "            print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward))\n",
        "\n",
        "            print_running_reward = 0\n",
        "            print_running_episodes = 0\n",
        "\n",
        "        # save model weights\n",
        "        if time_step % save_model_freq == 0:\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "            print(\"saving model at : \" + checkpoint_path)\n",
        "            ppo_agent.save(checkpoint_path)\n",
        "            print(\"model saved\")\n",
        "            print(\"Elapsed Time  : \", datetime.now().replace(microsecond=0) - start_time)\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "        # break; if the episode is over\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    print_running_reward += current_ep_reward\n",
        "    print_running_episodes += 1\n",
        "\n",
        "    log_running_reward += current_ep_reward\n",
        "    log_running_episodes += 1\n",
        "\n",
        "    i_episode += 1\n",
        "\n",
        "\n",
        "log_f.close()\n",
        "env.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print total training time\n",
        "print(\"============================================================================================\")\n",
        "end_time = datetime.now().replace(microsecond=0)\n",
        "print(\"Started training at (GMT) : \", start_time)\n",
        "print(\"Finished training at (GMT) : \", end_time)\n",
        "print(\"Total training time  : \", end_time - start_time)\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEy2qKdZF8ha"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "################################ End of Part II ################################\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHhK13_1G6zX"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - III**\n",
        "\n",
        "*   load and test preTrained networks on environments\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZWyhkq9Gxm5",
        "outputId": "70933929-aeab-4693-f4fe-8e32a59e334c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "loading network from : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode: 1 \t\t Reward: 238.0\n",
            "Episode: 2 \t\t Reward: 149.0\n",
            "Episode: 3 \t\t Reward: 105.0\n",
            "Episode: 4 \t\t Reward: 368.0\n",
            "Episode: 5 \t\t Reward: 400.0\n",
            "Episode: 6 \t\t Reward: 400.0\n",
            "Episode: 7 \t\t Reward: 274.0\n",
            "Episode: 8 \t\t Reward: 29.0\n",
            "Episode: 9 \t\t Reward: 87.0\n",
            "Episode: 10 \t\t Reward: 220.0\n",
            "============================================================================================\n",
            "average test reward : 227.0\n",
            "============================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "#################################### Testing ###################################\n",
        "\n",
        "\n",
        "################## hyperparameters ##################\n",
        "\n",
        "env_name = \"CartPole-v1\"\n",
        "has_continuous_action_space = False\n",
        "max_ep_len = 400\n",
        "action_std = None\n",
        "\n",
        "\n",
        "# env_name = \"LunarLander-v2\"\n",
        "# has_continuous_action_space = False\n",
        "# max_ep_len = 300\n",
        "# action_std = None\n",
        "\n",
        "\n",
        "# env_name = \"BipedalWalker-v2\"\n",
        "# has_continuous_action_space = True\n",
        "# max_ep_len = 1500           # max timesteps in one episode\n",
        "# action_std = 0.1            # set same std for action distribution which was used while saving\n",
        "\n",
        "\n",
        "# env_name = \"RoboschoolWalker2d-v1\"\n",
        "# has_continuous_action_space = True\n",
        "# max_ep_len = 1000           # max timesteps in one episode\n",
        "# action_std = 0.1            # set same std for action distribution which was used while saving\n",
        "\n",
        "\n",
        "total_test_episodes = 10    # total num of testing episodes\n",
        "\n",
        "K_epochs = 80               # update policy for K epochs\n",
        "eps_clip = 0.2              # clip parameter for PPO\n",
        "gamma = 0.99                # discount factor\n",
        "\n",
        "lr_actor = 0.0003           # learning rate for actor\n",
        "lr_critic = 0.001           # learning rate for critic\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# state space dimension\n",
        "state_dim = env.observation_space.shape[0]\n",
        "\n",
        "# action space dimension\n",
        "if has_continuous_action_space:\n",
        "    action_dim = env.action_space.shape[0]\n",
        "else:\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "\n",
        "# initialize a PPO agent\n",
        "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
        "\n",
        "\n",
        "# preTrained weights directory\n",
        "\n",
        "random_seed = 0             #### set this to load a particular checkpoint trained on random seed\n",
        "run_num_pretrained = 0      #### set this to load a particular checkpoint num\n",
        "\n",
        "\n",
        "directory = \"PPO_preTrained\" + '/' + env_name + '/'\n",
        "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
        "print(\"loading network from : \" + checkpoint_path)\n",
        "\n",
        "ppo_agent.load(checkpoint_path)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "\n",
        "test_running_reward = 0\n",
        "\n",
        "for ep in range(1, total_test_episodes+1):\n",
        "    ep_reward = 0\n",
        "    state = env.reset()\n",
        "\n",
        "    for t in range(1, max_ep_len+1):\n",
        "        action = ppo_agent.select_action(state)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        ep_reward += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # clear buffer\n",
        "    ppo_agent.buffer.clear()\n",
        "\n",
        "    test_running_reward +=  ep_reward\n",
        "    print('Episode: {} \\t\\t Reward: {}'.format(ep, round(ep_reward, 2)))\n",
        "    ep_reward = 0\n",
        "\n",
        "env.close()\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "avg_test_reward = test_running_reward / total_test_episodes\n",
        "avg_test_reward = round(avg_test_reward, 2)\n",
        "print(\"average test reward : \" + str(avg_test_reward))\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6IYC_JCGxlB"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "################################ End of Part III ###############################\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZewQELovHFt4"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - IV**\n",
        "\n",
        "*   load log files using pandas\n",
        "*   plot graph using matplotlib\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bY-E5HGcGxiu",
        "outputId": "93045a7b-dff0-44c8-9674-990c1c84c2d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "loading data from : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_0.csv\n",
            "data shape :  (125, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "loading data from : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_1.csv\n",
            "data shape :  (125, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "loading data from : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_2.csv\n",
            "data shape :  (125, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "loading data from : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_3.csv\n",
            "data shape :  (0, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "loading data from : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_4.csv\n",
            "data shape :  (0, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "loading data from : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_5.csv\n",
            "data shape :  (0, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "loading data from : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_6.csv\n",
            "data shape :  (0, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "loading data from : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_7.csv\n",
            "data shape :  (0, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "loading data from : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_8.csv\n",
            "data shape :  (125, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "loading data from : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_9.csv\n",
            "data shape :  (125, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "loading data from : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_10.csv\n",
            "data shape :  (125, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "============================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-42-879859696293>:77: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df_concat = pd.concat(all_runs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "figure saved at :  PPO_figs/CartPole-v1//PPO_CartPole-v1_fig_0.png\n",
            "============================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "env_name = 'CartPole-v1'\n",
        "# env_name = 'LunarLander-v2'\n",
        "# env_name = 'BipedalWalker-v2'\n",
        "# env_name = 'RoboschoolWalker2d-v1'\n",
        "\n",
        "\n",
        "fig_num = 0     #### change this to prevent overwriting figures in same env_name folder\n",
        "\n",
        "plot_avg = True    # plot average of all runs; else plot all runs separately\n",
        "\n",
        "fig_width = 10\n",
        "fig_height = 6\n",
        "\n",
        "\n",
        "# smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
        "window_len_smooth = 50\n",
        "min_window_len_smooth = 1\n",
        "linewidth_smooth = 1.5\n",
        "alpha_smooth = 1\n",
        "\n",
        "window_len_var = 5\n",
        "min_window_len_var = 1\n",
        "linewidth_var = 2\n",
        "alpha_var = 0.1\n",
        "\n",
        "\n",
        "colors = ['red', 'blue', 'green', 'orange', 'purple', 'olive', 'brown', 'magenta', 'cyan', 'crimson','gray', 'black']\n",
        "\n",
        "\n",
        "# make directory for saving figures\n",
        "figures_dir = \"PPO_figs\"\n",
        "if not os.path.exists(figures_dir):\n",
        "    os.makedirs(figures_dir)\n",
        "\n",
        "# make environment directory for saving figures\n",
        "figures_dir = figures_dir + '/' + env_name + '/'\n",
        "if not os.path.exists(figures_dir):\n",
        "    os.makedirs(figures_dir)\n",
        "\n",
        "\n",
        "fig_save_path = figures_dir + '/PPO_' + env_name + '_fig_' + str(fig_num) + '.png'\n",
        "\n",
        "\n",
        "# get number of log files in directory\n",
        "log_dir = \"PPO_logs\" + '/' + env_name + '/'\n",
        "\n",
        "current_num_files = next(os.walk(log_dir))[2]\n",
        "num_runs = len(current_num_files)\n",
        "\n",
        "\n",
        "all_runs = []\n",
        "\n",
        "for run_num in range(num_runs):\n",
        "\n",
        "    log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
        "    print(\"loading data from : \" + log_f_name)\n",
        "    data = pd.read_csv(log_f_name)\n",
        "    data = pd.DataFrame(data)\n",
        "\n",
        "    print(\"data shape : \", data.shape)\n",
        "\n",
        "    all_runs.append(data)\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "ax = plt.gca()\n",
        "\n",
        "if plot_avg:\n",
        "    # average all runs\n",
        "    df_concat = pd.concat(all_runs)\n",
        "    df_concat_groupby = df_concat.groupby(df_concat.index)\n",
        "    data_avg = df_concat_groupby.mean()\n",
        "\n",
        "    # smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
        "    data_avg['reward_smooth'] = data_avg['reward'].rolling(window=window_len_smooth, win_type='triang', min_periods=min_window_len_smooth).mean()\n",
        "    data_avg['reward_var'] = data_avg['reward'].rolling(window=window_len_var, win_type='triang', min_periods=min_window_len_var).mean()\n",
        "\n",
        "    data_avg.plot(kind='line', x='timestep' , y='reward_smooth',ax=ax,color=colors[0],  linewidth=linewidth_smooth, alpha=alpha_smooth)\n",
        "    data_avg.plot(kind='line', x='timestep' , y='reward_var',ax=ax,color=colors[0],  linewidth=linewidth_var, alpha=alpha_var)\n",
        "\n",
        "    # keep only reward_smooth in the legend and rename it\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    ax.legend([handles[0]], [\"reward_avg_\" + str(len(all_runs)) + \"_runs\"], loc=2)\n",
        "\n",
        "\n",
        "else:\n",
        "    for i, run in enumerate(all_runs):\n",
        "        # smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
        "        run['reward_smooth_' + str(i)] = run['reward'].rolling(window=window_len_smooth, win_type='triang', min_periods=min_window_len_smooth).mean()\n",
        "        run['reward_var_' + str(i)] = run['reward'].rolling(window=window_len_var, win_type='triang', min_periods=min_window_len_var).mean()\n",
        "\n",
        "        # plot the lines\n",
        "        run.plot(kind='line', x='timestep' , y='reward_smooth_' + str(i),ax=ax,color=colors[i % len(colors)],  linewidth=linewidth_smooth, alpha=alpha_smooth)\n",
        "        run.plot(kind='line', x='timestep' , y='reward_var_' + str(i),ax=ax,color=colors[i % len(colors)],  linewidth=linewidth_var, alpha=alpha_var)\n",
        "\n",
        "    # keep alternate elements (reward_smooth_i) in the legend\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    new_handles = []\n",
        "    new_labels = []\n",
        "    for i in range(len(handles)):\n",
        "        if(i%2 == 0):\n",
        "            new_handles.append(handles[i])\n",
        "            new_labels.append(labels[i])\n",
        "    ax.legend(new_handles, new_labels, loc=2)\n",
        "\n",
        "\n",
        "\n",
        "# ax.set_yticks(np.arange(0, 1800, 200))\n",
        "# ax.set_xticks(np.arange(0, int(4e6), int(5e5)))\n",
        "\n",
        "\n",
        "ax.grid(color='gray', linestyle='-', linewidth=1, alpha=0.2)\n",
        "\n",
        "ax.set_xlabel(\"Timesteps\", fontsize=12)\n",
        "ax.set_ylabel(\"Rewards\", fontsize=12)\n",
        "\n",
        "plt.title(env_name, fontsize=14)\n",
        "\n",
        "\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(fig_width, fig_height)\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "plt.savefig(fig_save_path)\n",
        "print(\"figure saved at : \", fig_save_path)\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaWPRW9EGxgH"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "################################ End of Part IV ################################\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8uG43MtHNGC"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - V**\n",
        "\n",
        "*   install virtual display libraries for rendering on colab / remote server ^\n",
        "*   load preTrained networks and save images for gif\n",
        "*   generate and save gif from previously saved images\n",
        "\n",
        "*   ^ If running locally; do not install xvbf and pyvirtualdisplay. Just comment out the virtual display code and render it normally.\n",
        "*   ^ You will still require to use ipythondisplay, if you want to render it in the Jupyter Notebook.\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VL3tpKf3HLAq"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#### to render on colab / server / headless machine install virtual display libraries\n",
        "\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5Rx_IFKHK-D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "outputId": "e4ce2e8b-519b-4f4b-e010-95b352777d65"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'roboschool'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-3c6e16788d01>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mroboschool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'roboschool'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "\n",
        "\n",
        "############################# save images for gif ##############################\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "import gym\n",
        "import roboschool\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "One frame corresponding to each timestep is saved in a folder :\n",
        "\n",
        "PPO_gif_images/env_name/000001.jpg\n",
        "PPO_gif_images/env_name/000002.jpg\n",
        "PPO_gif_images/env_name/000003.jpg\n",
        "...\n",
        "...\n",
        "...\n",
        "\n",
        "\n",
        "if this section is run multiple times or for multiple episodes for the same env_name;\n",
        "then the saved images will be overwritten.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### beginning of virtual display code section\n",
        "\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "\n",
        "#### end of virtual display code section\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "################## hyperparameters ##################\n",
        "\n",
        "env_name = \"CartPole-v1\"\n",
        "has_continuous_action_space = False\n",
        "max_ep_len = 400\n",
        "action_std = None\n",
        "\n",
        "\n",
        "# env_name = \"LunarLander-v2\"\n",
        "# has_continuous_action_space = False\n",
        "# max_ep_len = 300\n",
        "# action_std = None\n",
        "\n",
        "# env_name = \"BipedalWalker-v2\"\n",
        "# has_continuous_action_space = True\n",
        "# max_ep_len = 1500           # max timesteps in one episode\n",
        "# action_std = 0.1            # set same std for action distribution which was used while saving\n",
        "\n",
        "# env_name = \"RoboschoolWalker2d-v1\"\n",
        "# has_continuous_action_space = True\n",
        "# max_ep_len = 1000           # max timesteps in one episode\n",
        "# action_std = 0.1            # set same std for action distribution which was used while saving\n",
        "\n",
        "\n",
        "total_test_episodes = 1     # save gif for only one episode\n",
        "\n",
        "render_ipython = False      # plot the images using matplotlib and ipythondisplay before saving (slow)\n",
        "\n",
        "K_epochs = 80               # update policy for K epochs\n",
        "eps_clip = 0.2              # clip parameter for PPO\n",
        "gamma = 0.99                # discount factor\n",
        "\n",
        "lr_actor = 0.0003         # learning rate for actor\n",
        "lr_critic = 0.001         # learning rate for critic\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# state space dimension\n",
        "state_dim = env.observation_space.shape[0]\n",
        "\n",
        "# action space dimension\n",
        "if has_continuous_action_space:\n",
        "    action_dim = env.action_space.shape[0]\n",
        "else:\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "\n",
        "\n",
        "# make directory for saving gif images\n",
        "gif_images_dir = \"PPO_gif_images\" + '/'\n",
        "if not os.path.exists(gif_images_dir):\n",
        "    os.makedirs(gif_images_dir)\n",
        "\n",
        "# make environment directory for saving gif images\n",
        "gif_images_dir = gif_images_dir + '/' + env_name + '/'\n",
        "if not os.path.exists(gif_images_dir):\n",
        "    os.makedirs(gif_images_dir)\n",
        "\n",
        "# make directory for gif\n",
        "gif_dir = \"PPO_gifs\" + '/'\n",
        "if not os.path.exists(gif_dir):\n",
        "    os.makedirs(gif_dir)\n",
        "\n",
        "# make environment directory for gif\n",
        "gif_dir = gif_dir + '/' + env_name  + '/'\n",
        "if not os.path.exists(gif_dir):\n",
        "    os.makedirs(gif_dir)\n",
        "\n",
        "\n",
        "\n",
        "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
        "\n",
        "\n",
        "# preTrained weights directory\n",
        "\n",
        "random_seed = 0             #### set this to load a particular checkpoint trained on random seed\n",
        "run_num_pretrained = 0      #### set this to load a particular checkpoint num\n",
        "\n",
        "\n",
        "directory = \"PPO_preTrained\" + '/' + env_name + '/'\n",
        "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
        "print(\"loading network from : \" + checkpoint_path)\n",
        "\n",
        "ppo_agent.load(checkpoint_path)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "\n",
        "test_running_reward = 0\n",
        "\n",
        "for ep in range(1, total_test_episodes+1):\n",
        "\n",
        "    ep_reward = 0\n",
        "    state = env.reset()\n",
        "\n",
        "    for t in range(1, max_ep_len+1):\n",
        "        action = ppo_agent.select_action(state)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        ep_reward += reward\n",
        "\n",
        "        img = env.render(mode = 'rgb_array')\n",
        "\n",
        "\n",
        "        #### beginning of ipythondisplay code section 1\n",
        "\n",
        "        if render_ipython:\n",
        "            plt.imshow(img)\n",
        "            ipythondisplay.clear_output(wait=True)\n",
        "            ipythondisplay.display(plt.gcf())\n",
        "\n",
        "        #### end of ipythondisplay code section 1\n",
        "\n",
        "\n",
        "        img = Image.fromarray(img)\n",
        "        img.save(gif_images_dir + '/' + str(t).zfill(6) + '.jpg')\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # clear buffer\n",
        "    ppo_agent.buffer.clear()\n",
        "\n",
        "    test_running_reward +=  ep_reward\n",
        "    print('Episode: {} \\t\\t Reward: {}'.format(ep, round(ep_reward, 2)))\n",
        "    ep_reward = 0\n",
        "\n",
        "\n",
        "\n",
        "env.close()\n",
        "\n",
        "\n",
        "#### beginning of ipythondisplay code section 2\n",
        "\n",
        "if render_ipython:\n",
        "    ipythondisplay.clear_output(wait=True)\n",
        "\n",
        "#### end of ipythondisplay code section 2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "print(\"total number of frames / timesteps / images saved : \", t)\n",
        "\n",
        "avg_test_reward = test_running_reward / total_test_episodes\n",
        "avg_test_reward = round(avg_test_reward, 2)\n",
        "print(\"average test reward : \" + str(avg_test_reward))\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoVshl_ZHK7s"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "######################## generate gif from saved images ########################\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "env_name = 'CartPole-v1'\n",
        "# env_name = 'LunarLander-v2'\n",
        "# env_name = 'BipedalWalker-v2'\n",
        "# env_name = 'RoboschoolWalker2d-v1'\n",
        "\n",
        "\n",
        "gif_num = 0     #### change this to prevent overwriting gifs in same env_name folder\n",
        "\n",
        "# adjust following parameters to get desired duration, size (bytes) and smoothness of gif\n",
        "total_timesteps = 300\n",
        "step = 10\n",
        "frame_duration = 150\n",
        "\n",
        "\n",
        "# input images\n",
        "gif_images_dir = \"PPO_gif_images/\" + env_name + '/*.jpg'\n",
        "\n",
        "\n",
        "# ouput gif path\n",
        "gif_dir = \"PPO_gifs\"\n",
        "if not os.path.exists(gif_dir):\n",
        "    os.makedirs(gif_dir)\n",
        "\n",
        "gif_dir = gif_dir + '/' + env_name\n",
        "if not os.path.exists(gif_dir):\n",
        "    os.makedirs(gif_dir)\n",
        "\n",
        "gif_path = gif_dir + '/PPO_' + env_name + '_gif_' + str(gif_num) + '.gif'\n",
        "\n",
        "\n",
        "\n",
        "img_paths = sorted(glob.glob(gif_images_dir))\n",
        "img_paths = img_paths[:total_timesteps]\n",
        "img_paths = img_paths[::step]\n",
        "\n",
        "\n",
        "print(\"total frames in gif : \", len(img_paths))\n",
        "print(\"total duration of gif : \" + str(round(len(img_paths) * frame_duration / 1000, 2)) + \" seconds\")\n",
        "\n",
        "\n",
        "\n",
        "# save gif\n",
        "img, *imgs = [Image.open(f) for f in img_paths]\n",
        "img.save(fp=gif_path, format='GIF', append_images=imgs, save_all=True, optimize=True, duration=frame_duration, loop=0)\n",
        "\n",
        "print(\"saved gif at : \", gif_path)\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20d1bR8xHK5j"
      },
      "outputs": [],
      "source": [
        "\n",
        "############################# check gif byte size ##############################\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "env_name = 'CartPole-v1'\n",
        "# env_name = 'LunarLander-v2'\n",
        "# env_name = 'BipedalWalker-v2'\n",
        "# env_name = 'RoboschoolWalker2d-v1'\n",
        "\n",
        "\n",
        "gif_dir = \"PPO_gifs/\" + env_name + '/*.gif'\n",
        "\n",
        "gif_paths = sorted(glob.glob(gif_dir))\n",
        "\n",
        "for gif_path in gif_paths:\n",
        "    file_size = os.path.getsize(gif_path)\n",
        "    print(gif_path + '\\t\\t' + str(round(file_size / (1024 * 1024), 2)) + \" MB\")\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rM5UIAkcGxeA"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "################################# End of Part V ################################\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YUzQOu1HYHR"
      },
      "source": [
        "################################################################################\n",
        "\n",
        "---------------------------------------------------------------------------- That's all folks ! ----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "################################################################################"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "e7JowRQEGGKQ",
        "Z4VJcUT2GlJz"
      ],
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}